# GNN-Based Preconditioner for Sparse Matrices from r-Process Simulations

This repository contains a custom Graph Neural Network (GNN) developed to predict preconditioners for sparse matrices generated by r-process nucleosynthesis simulations using [SkyNet](https://bitbucket.org/jlippuner/skynet/). The GNN aims to accelerate the solution of large, sparse, and asymmetric linear systems by mimicking the effect of Incomplete LU (ILU)-based preconditioning.

This approach is inspired by recent work in machine learning for scientific computing, particularly:  
- [Jie Chen, 2025](https://arxiv.org/abs/2406.00809), *Graph Neural Preconditioners for Iterative Solutions of Sparse Linear Systems*  
- [Paul H√§usner et al., 2024](https://arxiv.org/abs/2305.16368), *Neural Incomplete Factorization: Learning Preconditioners for the Conjugate Gradient Method*


---

## üöÄ Objective

The GNN learns to generate preconditioners directly from the graph representation of a sparse Jacobian matrix. These learned preconditioners are used to enhance the convergence of iterative solvers, particularly BiCGSTAB, and in theory significantly reduce the computational cost of nucleosynthesis simulations.

To read in depth about this development and architecture of this code, please refer to my [Thesis](https://drive.google.com/file/d/1GhBuXnq0-GyOspdUU-tTIyZMzdo9vmVa/view?usp=sharing)

---

### üîÅ Preconditioner-Inverse Learning

- Instead of outputting an (N√óN) matrix like a traditional preconditioner, the GNN outputs an (N√ó1) solution vector, making it significantly more memory- and compute-efficient.
- The model is integrated into a **Flexible BiCGSTAB** solver, which supports variable preconditioners that change with each iteration ‚Äî ideal for neural approximations.

---

## üíæ Data

To test the GNN's capability, you modify the .ipynb to read the file with relevant CSR matrices. You can also test out the matrices generated by Skynet's r-process: [Drive link](https://drive.google.com/file/d/1cTpc-4QQu-jVxnd2h-GaCyFtsrJeu4sd/view?usp=drive_link)

---



## üìä Results

- Models trained using ILUT preconditioners generated via custom code and Intel MKL's `dcrilut`.
- Achieves effective post-preconditioning behaviour.
- Integration with iterative solvers significantly reduces residual norms and iteration counts.

---

## üîß Applications

- r-process simulations in astrophysics (SkyNet)
- Sparse matrix solvers in scientific computing
- Extendable to CFD, biological systems, and financial modelling

---


